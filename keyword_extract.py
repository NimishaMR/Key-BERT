# -*- coding: utf-8 -*-
"""keyword_extract.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-2MlS3Mt0E311prGf0k1O9BVpq48f1Rs
"""

import pandas as pd
df=pd.read_json("/content/dataset.json", lines=True)
df.head()

df=df.drop(['sentance','Arguments', 'Filename', 'Library', 'Resource'], axis='columns')

# df=df.drop(df.index[1000:1930])

df['Keyword'].nunique()

df.isnull().sum()

df.head()

"""LOWERCASE

"""

df['clean keyword']= df['Keyword'].str.lower()
df.sample(frac=1).head()

"""REMOVE PUNCTUATIONS

"""

import string
string.punctuation

def remove_punctuations(text):
  punctuations='!"#$%&\'()*+,-/:;<=>?@[\]^`{|}~'
  return text.translate(str.maketrans('','',punctuations))    
'''This uses the 3-argument version of str.maketrans
 with arguments (x, y, z) where 'x' and 'y'
 must be equal-length strings and characters in 'x'
 are replaced by characters in 'y'. 'z'
 is a string (string.punctuation here)
where each character in the string is mapped
 to None'''

df['clean keyword']=df['clean keyword'].apply(lambda x: remove_punctuations(x))               #x is nothing but the clean keywords

df.sample(frac=1).head(30)

"""REMOVAL OF STOPWORDS

"""

from nltk.tokenize import word_tokenize
from nltk.tokenize import sent_tokenize

from nltk.corpus import stopwords
import nltk
nltk.download('wordnet')
from nltk.stem import WordNetLemmatizer
nltk.download('punkt')
nltk.download("stopwords")
from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer

from nltk.corpus import wordnet
from nltk import pos_tag
from nltk import ne_chunk

from nltk.stem.porter import PorterStemmer
from nltk.stem.wordnet import WordNetLemmatizer

from nltk.probability import FreqDist
import matplotlib.pyplot as plt
from wordcloud import WordCloud
from nltk.corpus import stopwords
" ".join(stopwords.words('english'))

stop_words=set(stopwords.words('english'))

df['clean keyword'] = df['clean keyword'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))

df.sample(frac=1).head(30)

"""REMOVAL OF SPECIAL CHARACTERS"""

import re
def removesplchars(txt):
 txt=re.sub('[^a-zA-Z0-9._]',' ',txt)
 txt=re.sub('\s+',' ',txt)  #to replace additional not needed spaces with one space
 return txt

df['clean keyword']=df['clean keyword'].apply(lambda x: removesplchars(x))

df.sample(frac=1).head(30)

"""LEMMATIZATION AND POS TAGGING"""

from nltk import pos_tag

nltk.download('averaged_perceptron_tagger')
wordnet_map={'N':wordnet.NOUN,'V':wordnet.VERB,'J':wordnet.ADJ,'R':wordnet.ADV}
import nltk
nltk.download('wordnet')
from nltk.stem import WordNetLemmatizer
lemmatizer = WordNetLemmatizer()
def lemmatize_words(text):
  words = text.split()
  words = [lemmatizer.lemmatize(word,pos='v') for word in words]
  return ' '.join(words)

df['clean keyword']=df['clean keyword'].apply(lambda x:lemmatize_words(x))

df.sample(frac=1).head(20)

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# # !pip install keybert[all]
# !pip install keybert

#model


'''from keybert import KeyBERT
kw_model = KeyBERT()
keywords = df['clean keyword'].apply(kw_model.extract_keywords)
print(keywords[0])
print(keywords[29])'''

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !pip install bertopic
#

df["clean_keyword_len"] = df["clean keyword"].apply(lambda x : len(x.split()))
df.head(20)

df.clean_keyword_len.max() #longest input available

from bertopic import BERTopic

model = BERTopic(verbose=True,embedding_model='paraphrase-MiniLM-L6-v2', min_topic_size= 50)

pip install keybert

from keybert import KeyBERT

kw_model = KeyBERT()
doc_embeddings= df['clean keyword'].apply(lambda x: kw_model.extract_embeddings(x))

import numpy as np


df['output_key']=df['clean keyword'].apply(lambda x: kw_model.extract_keywords(x))

df['output_key']=df['clean keyword'].apply(lambda x: kw_model.extract_keywords(x, keyphrase_ngram_range=(1, 6), stop_words=None))
df.drop(['clean_keyword_len'], axis='columns')
df.sample(frac=1).head(20)

df['output_key2']=df['clean keyword'].apply(lambda x: kw_model.extract_keywords(x, keyphrase_ngram_range=(1, 6),
                              use_maxsum=True, nr_candidates=2, top_n=1))
df.sample(frac=1).head(20)

# df['output_key2']= (df['output_key']).values.tolist()
# df.sample(frac=1).head(20)



import pandas as pd
df1=pd.read_csv("input.txt", sep="\t", header=None, names=['input'] )
df1.head()

df1['input']

df1['clean']=df1['input'].str.lower()
df1['clean']=df1['input'].apply(lambda x: remove_punctuations(x))
df1['clean']=df1['clean'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))
df1['clean']=df1['clean'].apply(lambda x: removesplchars(x))
df1['clean']=df1['clean'].apply(lambda x:lemmatize_words(x))

df1

df1["clean_keyword_len2"] = df1["clean"].apply(lambda x : len(x.split()))
df1

df1.clean_keyword_len2.max() #longest input available

doc_embeddings= df1['clean'].apply(lambda x: kw_model.extract_embeddings(x))

df1['output_key2']=df1['clean'].apply(lambda x: kw_model.extract_keywords(x) )

df1['output_key2']=df1['clean'].apply(lambda x: kw_model.extract_keywords(x, keyphrase_ngram_range=(1,4), stop_words=None))
df1.drop(['clean_keyword_len2'], axis='columns')
df1

print(type(df['Keyword']))

df['Keyword'] = df["Keyword"].map(str)

df1

# !pip install rapidfuzz
# from rapidfuzz import process, fuzz

# df[['detach' in x for x in df['Keyword']]]

# df[df['Keyword'].str.contains(r'detach')]

# f = np.vectorize(lambda haystack, needle: needle in haystack)

# df[f(df['Keyword'], 'detach ue')]

# df['new']=df['Keyword'].filter(like='detach ue')

# re.search("detach ue")

# df.head()

"""To search for specific rows in the dataframe with specific strings"""

# df3 = df[df['clean keyword'].str.contains('verify')]
# print(df3)

# df3.drop(['Keyword','clean_keyword_len','output_key2'], axis=1)

# from difflib import SequenceMatcher
# def __len__(self):
#         return len(self)

# def similar(a, b):
#     la=__len__(a)
#     lb=__len(b)
#     return SequenceMatcher(None, a, b).ratio()

# input="verify site onair"
# df3['similarity']=similar(df['clean keyword'].str, input)

# input="verify site onair"
# df3 = pd.DataFrame({'clean keyword': ['verify site onair', 'verify site']})
# df3

# df['Code_e'] = df['Keyword'].str.extract(r'(\d+)').astype(int)
# df1['Code_e'] = df1['input'].str.extract(r'(\d+)').astype(int)
# final_df = pd.merge_asof(df1,df.sort_values(by='Code_e'),on='Code_e',suffixes=('','_right')).drop(['Code_e','Code_right'],axis=1)

!pip install fuzzywuzzy
from fuzzywuzzy import fuzz
from fuzzywuzzy import process

list1 = df['Keyword'].tolist()
list2 = df1['clean'].tolist()
mat1 = []
mat2 = []
p = []

list2

threshold = 85
for i in list2:
    mat1.append(process.extract(i, list1, limit=1))
df1['matches'] = mat1

mat1

for j in df1['matches']:
    for k in j:
        if k[1] >= threshold:
            p.append(k[0])
    mat2.append(",".join(p))
    p = []

mat2

df1['matches'] = mat2
print("\nDataFrame after Fuzzy matching:")
df1

for i in df1['matches']:
 print(df[df['Keyword']==i]['output_key'].item())

type(df['Keyword'])

type(df1['matches'])

mat3=[]
for i in df1['matches']:
    for j in df['Keyword']:
        if(df['Keyword'].equals(df1['matches'])):
            print('found')
            break
            # mat3.append(df['output_key2'])

df['output_key2']

for element in df1['matches']:
    # Search for the element in df2['Column3']
    matches = df[df['Keyword'] == element]

    # Print the matching rows (if any)
    if not matches.empty:
        print("Matches for element", element)
        print(matches['output_key2'])
        print("-----------------------")

